# Verified against live pod config — V2 benchmark run v2_run_20260217_125310
#
# KNOWN ISSUE: --mlock fails ("Cannot allocate memory" for 437 MB buffer).
# Needs memlock ulimit increase in Proxmox VM. Weights not pinned in RAM.
#
# KNOWN ISSUE: Speculative decoding partially broken. Slot 0 works, slot 1
# fails with "failed to create draft context" (insufficient VRAM for 2nd
# draft KV cache — only 2,217 MiB free). ~50% of requests get no spec decode.
#
# KNOWN ISSUE: --ctx-size 40960 with --parallel 2 = 20480 tokens per slot.
# Server logs: "n_ctx_seq (20480) < n_ctx_train (40960)".
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-server
  labels:
    app: llama-server
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: llama-server
  template:
    metadata:
      labels:
        app: llama-server
    spec:
      runtimeClassName: nvidia
      containers:
      - name: llama
        image: ${ATLAS_REGISTRY}/llama-server:${ATLAS_IMAGE_TAG}
        imagePullPolicy: Never
        ports:
        - containerPort: ${ATLAS_LLAMA_PORT}
        env:
        - name: MODEL_PATH
          value: "/models/${ATLAS_MAIN_MODEL}"
        - name: CONTEXT_LENGTH
          value: "${ATLAS_CONTEXT_LENGTH}"
        - name: GPU_LAYERS
          value: "${ATLAS_GPU_LAYERS}"
        - name: PARALLEL_SLOTS
          value: "${ATLAS_PARALLEL_SLOTS}"
        - name: DRAFT_MODEL
          value: "/models/${ATLAS_DRAFT_MODEL}"
        # PCIe transfer optimization: enable pinned (page-locked) host memory
        - name: GGML_CUDA_NO_PINNED
          value: "0"
        # Reduce CUDA context switch overhead
        - name: CUDA_DEVICE_MAX_CONNECTIONS
          value: "1"
        # Faster CUDA module startup
        - name: CUDA_MODULE_LOADING
          value: "LAZY"
        # KV cache quantization (reduces VRAM usage for KV cache)
        - name: KV_CACHE_TYPE
          value: "q4_0"
        # Custom Jinja chat template (mounted via ConfigMap)
        - name: CHAT_TEMPLATE
          value: "Qwen3-custom.jinja"
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: ${ATLAS_LLAMA_MEMORY_LIMIT}
            cpu: ${ATLAS_LLAMA_CPU_LIMIT}
          requests:
            memory: ${ATLAS_LLAMA_MEMORY_REQUEST}
            cpu: ${ATLAS_LLAMA_CPU_REQUEST}
        volumeMounts:
        - name: models
          mountPath: /models
        - name: lora
          mountPath: /models/lora
        - name: entrypoint
          mountPath: /entrypoint.sh
          subPath: entrypoint.sh
        - name: templates
          mountPath: /templates
      volumes:
      - name: models
        hostPath:
          path: ${ATLAS_MODELS_DIR}
          type: Directory
      - name: lora
        hostPath:
          path: ${ATLAS_LORA_DIR}
          type: DirectoryOrCreate
      - name: entrypoint
        configMap:
          name: llama-entrypoint
          defaultMode: 0755
      - name: templates
        configMap:
          name: llama-templates
          defaultMode: 0644
---
apiVersion: v1
kind: Service
metadata:
  name: llama-service
spec:
  selector:
    app: llama-server
  ports:
  - port: ${ATLAS_LLAMA_PORT}
    targetPort: ${ATLAS_LLAMA_PORT}
    nodePort: ${ATLAS_LLAMA_NODEPORT}
  type: NodePort
