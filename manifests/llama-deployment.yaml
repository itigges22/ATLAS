# Verified against live pod config â€” V2.0.0
#
# NOTE: --ctx-size 16384 with --parallel 2 = 8192 tokens per slot.
# This is inherent to running 2 parallel slots for Best-of-K pipelining.
#
# mlock and speculative decoding on both slots require LimitMEMLOCK=infinity
# in the K3s systemd service (see /etc/systemd/system/k3s.service.d/memlock.conf).
# Draft KV cache uses q4_0 quantization (-ctkd/-ctvd) to fit both draft contexts in VRAM.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-server
  labels:
    app: llama-server
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: llama-server
  template:
    metadata:
      labels:
        app: llama-server
    spec:
      runtimeClassName: nvidia
      containers:
      - name: llama
        image: localhost/llama-server:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_PATH
          value: "/models/Qwen3-14B-Q4_K_M.gguf"
        - name: CONTEXT_LENGTH
          value: "16384"
        - name: GPU_LAYERS
          value: "99"
        - name: PARALLEL_SLOTS
          value: "2"
        - name: DRAFT_MODEL
          value: "/models/Qwen3-0.6B-Q8_0.gguf"
        # PCIe transfer optimization: enable pinned (page-locked) host memory
        - name: GGML_CUDA_NO_PINNED
          value: "0"
        # Reduce CUDA context switch overhead
        - name: CUDA_DEVICE_MAX_CONNECTIONS
          value: "1"
        # Faster CUDA module startup
        - name: CUDA_MODULE_LOADING
          value: "LAZY"
        # KV cache quantization (reduces VRAM usage for KV cache)
        - name: KV_CACHE_TYPE
          value: "q4_0"
        # Custom Jinja chat template (mounted via ConfigMap)
        - name: CHAT_TEMPLATE
          value: "Qwen3-custom.jinja"
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: 16Gi
            cpu: 4
          requests:
            memory: 8Gi
            cpu: 2
        volumeMounts:
        - name: models
          mountPath: /models
        - name: lora
          mountPath: /models/lora
        - name: entrypoint
          mountPath: /entrypoint.sh
          subPath: entrypoint.sh
        - name: templates
          mountPath: /templates
      # Sidecar: nomic-embed-text-v1.5 embedding server (shares pod GPU)
      - name: llama-embed
        image: localhost/llama-server:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 8001
        resources:
          limits:
            memory: 2Gi
            cpu: 1
          requests:
            memory: 512Mi
            cpu: 100m
        volumeMounts:
        - name: models
          mountPath: /models
        - name: entrypoint
          mountPath: /entrypoint.sh
          subPath: entrypoint-embed.sh
      volumes:
      - name: models
        hostPath:
          # CHANGE THIS: Set to your model directory (see docs/SETUP.md)
          path: /home/isaac/models
          type: Directory
      - name: lora
        hostPath:
          # CHANGE THIS: Set to your model directory + /lora (see docs/SETUP.md)
          path: /home/isaac/models/lora
          type: DirectoryOrCreate
      - name: entrypoint
        configMap:
          name: llama-entrypoint
          defaultMode: 0755
      - name: templates
        configMap:
          name: llama-templates
          defaultMode: 0644
---
apiVersion: v1
kind: Service
metadata:
  name: llama-service
spec:
  selector:
    app: llama-server
  ports:
  - port: 8000
    targetPort: 8000
    nodePort: 32735
  type: NodePort
